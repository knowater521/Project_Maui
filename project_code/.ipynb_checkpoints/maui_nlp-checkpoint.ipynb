{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP of news API sentiment analysis. Disney Ex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iexfinance.stocks import Stock\n",
    "import pandas as pd\n",
    "from newsapi.newsapi_client import NewsApiClient\n",
    "from datetime import date, datetime, timedelta\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pickle\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "nltk.download('vader_lexicon')\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsapi = NewsApiClient(api_key=os.environ[\"NEWS_API\"])\n",
    "pickle.dump(newsapi,open('newsapi.pickle','wb'))\n",
    "current_date = date(2020,4,13)\n",
    "past_date = date(2020,4,13) - timedelta(days=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headlines(keyword):\n",
    "    all_headlines = []  #empty list for all headlines\n",
    "    all_dates = [] #empyt list for all dates\n",
    "    date = current_date #today's date\n",
    "    print(f\"Fetching news about '{keyword}'\") #prints single string of function starting\n",
    "    print(\"*\" * 30) #creating a line of stars for readability\n",
    "    while date > past_date: #establishes length of dates being pulled by for the lenght of the difference between today and past_date \n",
    "        print(f\"retrieving news from: {date}\")#printing string for loop readability  \n",
    "        articles = newsapi.get_everything(   #pulling articles through API\n",
    "            q=keyword,\n",
    "            from_param=str(date),\n",
    "            to=str(date),\n",
    "            language=\"en\",\n",
    "            sort_by=\"relevancy\",\n",
    "            page=1,\n",
    "        )\n",
    "        headlines = []\n",
    "        for i in range(0, len(articles[\"articles\"])):\n",
    "            \n",
    "            #pulling the content part of the dict\n",
    "            headlines.append(articles[\"articles\"][i][\"content\"])\n",
    "        all_headlines.append(headlines)\n",
    "        all_dates.append(date)\n",
    "        date = date - timedelta(days=1) #moving through the days            \n",
    "    return all_headlines, all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_headlines, dates = get_headlines('disney')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = pd.DataFrame(company_headlines)\n",
    "company_df.index = pd.to_datetime(dates)\n",
    "company_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concatinated_list = []\n",
    "i = 0\n",
    "while i < len(company_df):\n",
    "    date = company_df.index[i]\n",
    "    daily_data = company_df.iloc[i,:].dropna().to_list()\n",
    "    daily_data = \"\".join(daily_data)\n",
    "    concatinated_list.append({\"Date\":date,\n",
    "                              \"articles\":daily_data\n",
    "                              })\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df = pd.DataFrame(concatinated_list)\n",
    "company_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', text)\n",
    "    words = word_tokenize(re_clean)\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    tokens = [word.lower() for word in lem if word.lower() not in sw]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_df[\"tokens\"] = [tokenizer(i) for i in company_df[\"articles\"]]\n",
    "company_df[\"Clean Words\"] = [\" \".join(i)for i in company_df[\"tokens\"]]    \n",
    "company_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(dataframe):\n",
    "# list of sentiment objects observed\n",
    "    df_sentiments = []\n",
    "    i = 0 \n",
    "    # for rows in df: # the next time you put a for-loop outside of a try fn there will be a muckduck\n",
    "    # for loops are used for interating through one object ie. for every item in list \n",
    "    # while loops can be used for interating through multiple objects on the same index ie. 1st item in ...for 2nd item in... for 3rd item in.. \n",
    "\n",
    "\n",
    "    while i < len(dataframe[\"Clean Words\"]): # if we did len of df, then it would be the length of columns\n",
    "                                  # by doing len of df[col], then its length of rows\n",
    "\n",
    "        # to get values for the sentiment parameters\n",
    "        text = dataframe[\"Clean Words\"][i] \n",
    "        date = dataframe[\"Date\"][i]\n",
    "\n",
    "        # activate sentiment analysis\n",
    "        sentiment = analyzer.polarity_scores(text)  ## Attribute Error is applied for NoneTypes in \n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "\n",
    "        # append results of sentiment analysis per row of sentiment parameters df\n",
    "        df_sentiments.append({\n",
    "                \"text\": text,\n",
    "                \"date\": date,\n",
    "                \"compound\": compound,\n",
    "                \"positive\": pos,\n",
    "                \"negative\": neg,\n",
    "                \"neutral\": neu\n",
    "\n",
    "            })\n",
    "\n",
    "\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Create DataFrame\n",
    "    final_df = pd.DataFrame(df_sentiments)\n",
    "\n",
    "    # Reorder DataFrame columns\n",
    "    cols = [\"date\", \"text\", \"compound\", \"positive\", \"negative\", \"neutral\"]\n",
    "    final_list = final_df[cols]\n",
    "\n",
    "    return final_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_sentiment = get_sentiment(company_df)\n",
    "company_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
