{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iexfinance.stocks import Stock\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from newsapi.newsapi_client import NewsApiClient\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import date, datetime, timedelta\n",
    "import os \n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "from iexfinance.stocks import get_historical_data\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from string import punctuation\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Stock Selection\n",
    "\n",
    "## This file defines the stocks that a user can select to feed into the program. Stock selection is driven from a dropdown menu that passes a list as an output. This list will be fed into the IEX Finance and Reuters News API queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3M Company</td>\n",
       "      <td>MMM</td>\n",
       "      <td>3M Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>A.O. Smith Corp</td>\n",
       "      <td>AOS</td>\n",
       "      <td>A.O. Smith Corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AbbVie Inc.</td>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Accenture plc</td>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture plc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Symbol                 Name\n",
       "Company                                        \n",
       "3M Company             MMM           3M Company\n",
       "A.O. Smith Corp        AOS      A.O. Smith Corp\n",
       "Abbott Laboratories    ABT  Abbott Laboratories\n",
       "AbbVie Inc.           ABBV          AbbVie Inc.\n",
       "Accenture plc          ACN        Accenture plc"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing list of companies and converting to DataFrame\n",
    "sp500_csv = Path(\"Data/sp500_constituents.csv\")\n",
    "sp500_df = pd.read_csv(sp500_csv)\n",
    "sp500_df['Company'] = sp500_df['Name']\n",
    "sp500_df.drop(columns='Sector',inplace=True)\n",
    "sp500_df.set_index('Company', inplace=True)\n",
    "sp500_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting DataFrame to a dictionary of lists\n",
    "stock_dict = sp500_df.T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an interactive widget that allows the user to select a company\n",
    "selector_widget = widgets.Dropdown(\n",
    "    options=list(stock_dict.keys()),\n",
    "    continuous_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3M Company'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the output of the selector value as a variable\n",
    "selector_widget.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Get Model Input Data\n",
    "\n",
    "## This section takes the output from the stock selection to pull historical stock data and relevant news. With this data, we calculate daily stock returns for ML model targets, and runan NLP sentiment analysis model for ML model features.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Richard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to pull stock prices to calculate return prices, and pull news articles to caluculate sentiment\n",
    "\n",
    "def get_model_data(company, lag=0):\n",
    "    \n",
    "    sp500_csv = Path(\"Data/sp500_constituents.csv\")\n",
    "    sp500_df = pd.read_csv(sp500_csv)\n",
    "    sp500_df['Company'] = sp500_df['Name']\n",
    "    sp500_df.drop(columns='Sector',inplace=True)\n",
    "    sp500_df.set_index('Company', inplace=True)\n",
    "    \n",
    "    new_stock_dict = sp500_df.T.to_dict('list')\n",
    "    \n",
    "    from datetime import date, datetime, timedelta\n",
    "    newsapi = NewsApiClient(api_key=os.environ[\"NEWS_API\"])\n",
    "    pickle.dump(newsapi,open('newsapi.pickle','wb'))\n",
    "    current_date = date.today()\n",
    "    past_date = date.today() - timedelta(days=30)\n",
    "    \n",
    "    def get_headlines(keyword):\n",
    "        \n",
    "        # empty list for all headlines\n",
    "        all_headlines = []  \n",
    "        # empty list for all dates\n",
    "        all_dates = [] \n",
    "        # today's date\n",
    "        date = current_date \n",
    "        \n",
    "        # establishes length of dates being pulled by for the length of the difference between today and past_date \n",
    "        while date > past_date:  \n",
    "            \n",
    "            # pulling articles through API\n",
    "            articles = newsapi.get_everything(   \n",
    "                q=keyword,\n",
    "                from_param=str(date),\n",
    "                to=str(date),\n",
    "                language=\"en\",\n",
    "                sort_by=\"relevancy\",\n",
    "                page=1,\n",
    "            )\n",
    "            headlines = []\n",
    "            \n",
    "            for i in range(0, len(articles[\"articles\"])):\n",
    "\n",
    "                # pulling the content part of the dict\n",
    "                headlines.append(articles[\"articles\"][i][\"content\"])\n",
    "                \n",
    "            all_headlines.append(headlines)\n",
    "            all_dates.append(date)\n",
    "            \n",
    "            # moving through the days\n",
    "            date = date - timedelta(days=1)   \n",
    "            \n",
    "        return all_headlines, all_dates\n",
    "    \n",
    "    headlines, dates = get_headlines(new_stock_dict[company][1])\n",
    "    \n",
    "    df = pd.DataFrame(headlines)\n",
    "    df.index = pd.to_datetime(dates)\n",
    "    \n",
    "    concatinated_list = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(df):\n",
    "        date = df.index[i]\n",
    "        daily_data = df.iloc[i,:].dropna().to_list()\n",
    "        daily_data = \"\".join(daily_data)\n",
    "        concatinated_list.append({\"Date\":date,\n",
    "                                  \"articles\":daily_data\n",
    "                                  })\n",
    "        i = i + 1\n",
    "        \n",
    "    df = pd.DataFrame(concatinated_list)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def tokenizer(text):\n",
    "        sw = set(stopwords.words('english'))\n",
    "        regex = re.compile(\"[^a-zA-Z ]\")\n",
    "        re_clean = regex.sub('', text)\n",
    "        words = word_tokenize(re_clean)\n",
    "        lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "        tokens = [word.lower() for word in lem if word.lower() not in sw]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    df[\"tokens\"] = [tokenizer(i) for i in df[\"articles\"]]\n",
    "    df[\"Clean Words\"] = [\" \".join(i)for i in df[\"tokens\"]]\n",
    "    \n",
    "    def get_sentiment(dataframe):\n",
    "        \n",
    "        # list of sentiment objects observed\n",
    "        df_sentiments = []\n",
    "        i = 0 \n",
    "\n",
    "        # if we did len of df, then it would be the length of columns by doing len of df[col], then its length of rows\n",
    "        while i < len(dataframe[\"Clean Words\"]): \n",
    "\n",
    "            # to get values for the sentiment parameters\n",
    "            text = dataframe[\"Clean Words\"][i] \n",
    "            date = dataframe[\"Date\"][i]\n",
    "\n",
    "            # activate sentiment analysis | Attribute Error is applied for | NoneTypes in \n",
    "            sentiment = analyzer.polarity_scores(text) \n",
    "                                                        \n",
    "            compound = sentiment[\"compound\"]\n",
    "            pos = sentiment[\"pos\"]\n",
    "            neu = sentiment[\"neu\"]\n",
    "            neg = sentiment[\"neg\"]\n",
    "\n",
    "            # append results of sentiment analysis per row of sentiment parameters df\n",
    "            df_sentiments.append({\n",
    "                    \"text\": text,\n",
    "                    \"date\": date,\n",
    "                    \"compound\": compound,\n",
    "                    \"positive\": pos,\n",
    "                    \"negative\": neg,\n",
    "                    \"neutral\": neu\n",
    "                })\n",
    "            i += 1\n",
    "\n",
    "        # create DataFrame\n",
    "        final_df = pd.DataFrame(df_sentiments)\n",
    "\n",
    "        # reorder DataFrame columns\n",
    "        cols = [\"date\", \"text\", \"compound\", \"positive\", \"negative\", \"neutral\"]\n",
    "        final_list = final_df[cols]\n",
    "\n",
    "        return final_list\n",
    "\n",
    "    df = get_sentiment(df)\n",
    "    df = df.set_index('date')\n",
    "    df = df.sort_values(by='date',ascending=True)\n",
    "\n",
    "    # setting start and end date for the past four weeks\n",
    "    # 31 days needed instead of 30 days so that we get 30 days of return when we calculate\n",
    "    end_date_stock = datetime.now()\n",
    "    start_date_stock = end_date_stock + timedelta(-31)\n",
    "\n",
    "    # getting data from the API and adding to DataFrame\n",
    "    returns_df = get_historical_data(new_stock_dict[company][0], start_date_stock, end_date_stock,  \n",
    "                                     output_format='pandas')\n",
    "    returns_df.drop(columns=['open','high','low','volume'],inplace=True)\n",
    "    returns_df = returns_df.pct_change() * 100\n",
    "    returns_df.dropna(inplace=True)\n",
    "    returns_df.isnull().sum()\n",
    "    returns_df.rename(columns={'close':'return'}, inplace=True)\n",
    "    \n",
    "    combined_df = pd.concat([df, returns_df], axis=1)\n",
    "\n",
    "    # function takes in an NLP/stock returns dataframe and calculates the average polarity\n",
    "    # scores over non-trading days and replaces scores of the first trading day after\n",
    "    # non-trading days with average score of previous days.\n",
    "    def cleaned_df(dataframe):\n",
    "        compound = []\n",
    "        positive = []\n",
    "        negative = []\n",
    "        neutral = []\n",
    "\n",
    "        for index, row in dataframe.iterrows():\n",
    "\n",
    "            # If daily return is null value for a given day, append polarity scores to their\n",
    "            # respective lists.\n",
    "            if pd.isnull(row['return']):\n",
    "                compound.append(row['compound'])\n",
    "                positive.append(row['positive'])\n",
    "                negative.append(row['negative'])\n",
    "                neutral.append(row['neutral'])\n",
    "                dataframe.drop(index=index, inplace=True)\n",
    "\n",
    "            elif pd.notnull(row['return']):\n",
    "                \n",
    "                # The list of compound polarity scores will be empty if the stock was traded\n",
    "                # on the previous day; therefore, move along.\n",
    "                if len(compound) == 0:\n",
    "                    pass\n",
    "\n",
    "                # If the list is not empty, then at least one day prior was a non-trading \n",
    "                # day. Append the current day's scores to the list and calculate the mean \n",
    "                # for each score. Then replace the current day's polarity scores with the \n",
    "                # average scores of today and previous non-trading days.\n",
    "                else:\n",
    "                    compound.append(row['compound'])\n",
    "                    compound_mean = np.mean(compound)\n",
    "                    compound = []\n",
    "\n",
    "                    positive.append(row['positive'])\n",
    "                    positive_mean = np.mean(positive)\n",
    "                    positive = []\n",
    "\n",
    "                    negative.append(row['negative'])\n",
    "                    negative_mean = np.mean(negative)\n",
    "                    negative = []\n",
    "\n",
    "                    neutral.append(row['neutral'])\n",
    "                    neutral_mean = np.mean(neutral)\n",
    "                    neutral = []\n",
    "\n",
    "                    dataframe.at[index, 'compound'] = compound_mean\n",
    "                    dataframe.at[index, 'positive'] = positive_mean\n",
    "                    dataframe.at[index, 'negative'] = negative_mean\n",
    "                    dataframe.at[index, 'neutral'] = neutral_mean\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return dataframe.sort_index(ascending=True)\n",
    "    \n",
    "    # Shift the return column up to adjust for a lag in stock reaction to sentiments.\n",
    "    final_df = cleaned_df(combined_df)\n",
    "    final_df['return'] = final_df['return'].shift(-lag)\n",
    "    final_df.dropna(inplace=True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Machine Learning Model\n",
    "\n",
    "## This section takes in the news sentiment data as features and the stock price returns as targets, and feeds them into a machine learning model. The model will output predicted price movement, and model accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing ML model libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import linear_model \n",
    "from imblearn.metrics import classification_report_imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(df):\n",
    "\n",
    "    # Prepare the dataframe\n",
    "    df['return_sign'] = np.sign(df['return'].values)\n",
    "    df = df.drop(columns=['text'])\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Create the features (X) and target (y) sets\n",
    "    X = df.iloc[:, 0:4]\n",
    "    y = df[\"return_sign\"]\n",
    "    \n",
    "    # creating training and testing data sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle=False, random_state=42) \n",
    "    \n",
    "    # fitting model\n",
    "    M = 'Logit'\n",
    "    lm = linear_model.LogisticRegression(solver = 'lbfgs')\n",
    "    lm.fit(X_train, y_train)\n",
    "    lm_pred = lm.predict(X_test)\n",
    "    \n",
    "    ## evaluating model\n",
    "    \n",
    "    # calculating the confusion matrix\n",
    "    cm_lm = confusion_matrix(y_test, lm_pred)\n",
    "    cm_lm_df = pd.DataFrame(\n",
    "    cm_lm, index=[\"Actual -1\", \"Actual 1\"], columns=[\"Predicted -1\", \"Predicted 1\"]\n",
    "    )\n",
    "    \n",
    "    # calculating the accuracy score\n",
    "    acc_lm_score = balanced_accuracy_score(y_test, lm_pred)\n",
    "    \n",
    "    return acc_lm_score, lm_pred[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Buy/Sell Recommendations\n",
    "\n",
    "## This section is used to create the conditional statements that will display the outputs of the ML model, and offer buy/sell recommendations based on them. The outputs and recommendation will be displayed in a widget that will be exported to a panel dashboard along with the input widget to form the user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating accuracy selector widget for setting model accuracy threshold to feed into conditional statements\n",
    "accuracy_selector = widgets.IntSlider(\n",
    "    value=75,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4804e062e045b6b479ee8e8cef9115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=75)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# saving the output of the accuracy value as a variable\n",
    "accuracy_selector.value\n",
    "\n",
    "display(accuracy_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating conditional statement to determine buy/sell recommendations\n",
    "def conditionals(accuracy, predicted, model_df):\n",
    "    if (accuracy*100) > accuracy_selector.value:\n",
    "\n",
    "        if predicted == 1:\n",
    "            output = f'{selector_widget.value}: With a composite news sentiment score of {model_df.iloc[-1][1]}, there is a {accuracy*100}% chance there will be a price increase. Our recommendation: BUY.'\n",
    "        else:\n",
    "            output = f'{selector_widget.value}: With a composite news sentiment score of {model_df.iloc[-1][1]}, there is a {accuracy*100}% chance there will be a price decrease. Our recommendation: SELL.'   \n",
    "\n",
    "    else: \n",
    "        output = f'{selector_widget.value}: Model accuracy is only {accuracy*100}%, which does not meet your confidence threshold. We cannot provide an investment recommendation given this uncertainty.'\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: User Interface\n",
    "\n",
    "## This section is used to create the ipywidgets that will be used to trigger running the model based on user inputs, and display the model outputs + recommendations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = widgets.Layout(border='solid 1.5px')\n",
    "# creating output widget\n",
    "output_text = widgets.Output(layout=layout)\n",
    "\n",
    "#output_text.append_stdout(output)\n",
    "#with output_text:\n",
    "    \n",
    "\n",
    "recommendation_title = widgets.Output(layout=layout)\n",
    "recommendation_title.append_stdout('Your Recommendation:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating input widget\n",
    "stock_selection = selector_widget.value\n",
    "\n",
    "keyword = f'{stock_selection} AND {stock_selection}'\n",
    "\n",
    "#stock_selection[1]\n",
    "\n",
    "selector_title = widgets.Output(layout=layout)\n",
    "selector_title.append_stdout('Choose Company:')\n",
    "\n",
    "accuracy_title = widgets.Output(layout=layout)\n",
    "accuracy_title.append_stdout('Required Model Accuracy (%):')\n",
    "\n",
    "run_button = widgets.Button(description=\"Run Model\",layout=layout)\n",
    "\n",
    "# TODO - update this function so that the whole model runs again with the new inputs when the user clicks the button\n",
    "def on_button_clicked(b):\n",
    "    \n",
    "    model_input_df = get_model_data(selector_widget.value)\n",
    "    \n",
    "    \n",
    "    acc_lm_score, lm_pred = model(model_input_df)\n",
    "    \n",
    "    with output_text:\n",
    "        clear_output()\n",
    "        con = conditionals(acc_lm_score, lm_pred, model_input_df)\n",
    "    output_text.append_stdout(con)\n",
    "    \n",
    "\n",
    "    \n",
    "run_button.on_click(on_button_clicked)\n",
    "\n",
    "input_widget = widgets.VBox([selector_title, selector_widget, \n",
    "                              accuracy_title, accuracy_selector, \n",
    "                              run_button],\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659d6984cb0f4c54af06660f53e9d98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display widgets\n",
    "final_widget = widgets.Output()\n",
    "with final_widget:\n",
    "    bigbox = widgets.VBox([input_widget,recommendation_title, output_text])\n",
    "    display(bigbox)\n",
    "    \n",
    "final_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
