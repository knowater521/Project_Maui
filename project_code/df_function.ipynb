{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iexfinance.stocks import Stock\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from newsapi.newsapi_client import NewsApiClient\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from datetime import date, datetime, timedelta\n",
    "import os \n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "from iexfinance.stocks import get_historical_data\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Richard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(ticker, lag=0):\n",
    "    \n",
    "    sp500_csv = Path(\"project_code/Data/sp500_constituents.csv\")\n",
    "    sp500_df = pd.read_csv(sp500_csv)\n",
    "    sp500_df['Ticker'] = sp500_df['Symbol']\n",
    "    sp500_df.drop(columns='Sector',inplace=True)\n",
    "    sp500_df.set_index('Ticker', inplace=True)\n",
    "    \n",
    "    stock_dict = sp500_df.T.to_dict('list')\n",
    "    \n",
    "    from datetime import date, datetime, timedelta\n",
    "    newsapi = NewsApiClient(api_key=os.environ[\"NEWS_API\"])\n",
    "    pickle.dump(newsapi,open('newsapi.pickle','wb'))\n",
    "    current_date = date.today()\n",
    "    past_date = date.today() - timedelta(days=30)\n",
    "    \n",
    "    def get_headlines(keyword):\n",
    "        all_headlines = []  #empty list for all headlines\n",
    "        all_dates = [] #empyt list for all dates\n",
    "        date = current_date #today's date\n",
    "        #print(f\"Fetching news about '{keyword}'\") #prints single string of function starting\n",
    "        #print(\"*\" * 30) #creating a line of stars for readability\n",
    "        while date > past_date: #establishes length of dates being pulled by for the \n",
    "            #lenght of the difference between today and past_date \n",
    "            #print(f\"retrieving news from: {date}\")#printing string for loop readability  \n",
    "            articles = newsapi.get_everything(   #pulling articles through API\n",
    "                q=keyword,\n",
    "                from_param=str(date),\n",
    "                to=str(date),\n",
    "                language=\"en\",\n",
    "                sort_by=\"relevancy\",\n",
    "                page=1,\n",
    "            )\n",
    "            headlines = []\n",
    "            for i in range(0, len(articles[\"articles\"])):\n",
    "\n",
    "                #pulling the content part of the dict\n",
    "                headlines.append(articles[\"articles\"][i][\"content\"])\n",
    "            all_headlines.append(headlines)\n",
    "            all_dates.append(date)\n",
    "            date = date - timedelta(days=1) #moving through the days            \n",
    "        return all_headlines, all_dates\n",
    "    \n",
    "    headlines, dates = get_headlines(stock_dict[ticker][1])\n",
    "    \n",
    "    df = pd.DataFrame(headlines)\n",
    "    df.index = pd.to_datetime(dates)\n",
    "    \n",
    "    concatinated_list = []\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        date = df.index[i]\n",
    "        daily_data = df.iloc[i,:].dropna().to_list()\n",
    "        daily_data = \"\".join(daily_data)\n",
    "        concatinated_list.append({\"Date\":date,\n",
    "                                  \"articles\":daily_data\n",
    "                                  })\n",
    "        i = i + 1\n",
    "        \n",
    "    df = pd.DataFrame(concatinated_list)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def tokenizer(text):\n",
    "        sw = set(stopwords.words('english'))\n",
    "        regex = re.compile(\"[^a-zA-Z ]\")\n",
    "        re_clean = regex.sub('', text)\n",
    "        words = word_tokenize(re_clean)\n",
    "        lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "        tokens = [word.lower() for word in lem if word.lower() not in sw]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    df[\"tokens\"] = [tokenizer(i) for i in df[\"articles\"]]\n",
    "    df[\"Clean Words\"] = [\" \".join(i)for i in df[\"tokens\"]]\n",
    "    \n",
    "    def get_sentiment(dataframe):\n",
    "    # list of sentiment objects observed\n",
    "        df_sentiments = []\n",
    "        i = 0 \n",
    "        # for rows in df: # the next time you put a for-loop outside of a try fn \n",
    "        # there will be a muckduck\n",
    "        # for loops are used for interating through one object ie. for every item in list \n",
    "        # while loops can be used for interating through multiple objects on the same \n",
    "        # index ie. 1st item in ...for 2nd item in... for 3rd item in.. \n",
    "\n",
    "\n",
    "        while i < len(dataframe[\"Clean Words\"]): # if we did len of df, then it would be \n",
    "            # the length of columns by doing len of df[col], then its length of rows\n",
    "\n",
    "            # to get values for the sentiment parameters\n",
    "            text = dataframe[\"Clean Words\"][i] \n",
    "            date = dataframe[\"Date\"][i]\n",
    "\n",
    "            # activate sentiment analysis\n",
    "            sentiment = analyzer.polarity_scores(text)  ## Attribute Error is applied for \n",
    "                                                        ## NoneTypes in \n",
    "            compound = sentiment[\"compound\"]\n",
    "            pos = sentiment[\"pos\"]\n",
    "            neu = sentiment[\"neu\"]\n",
    "            neg = sentiment[\"neg\"]\n",
    "\n",
    "            # append results of sentiment analysis per row of sentiment parameters df\n",
    "            df_sentiments.append({\n",
    "                    \"text\": text,\n",
    "                    \"date\": date,\n",
    "                    \"compound\": compound,\n",
    "                    \"positive\": pos,\n",
    "                    \"negative\": neg,\n",
    "                    \"neutral\": neu\n",
    "                })\n",
    "            i += 1\n",
    "\n",
    "        # Create DataFrame\n",
    "        final_df = pd.DataFrame(df_sentiments)\n",
    "\n",
    "        # Reorder DataFrame columns\n",
    "        cols = [\"date\", \"text\", \"compound\", \"positive\", \"negative\", \"neutral\"]\n",
    "        final_list = final_df[cols]\n",
    "\n",
    "        return final_list\n",
    "\n",
    "    df = get_sentiment(df)\n",
    "    df = df.set_index('date')\n",
    "    df = df.sort_values(by='date',ascending=True)\n",
    "\n",
    "    # setting start and end date for the past four weeks\n",
    "    # 29 days needed instead of 28 days so that we get 28 days of return when we calculate\n",
    "    end_date_stock = datetime.now()\n",
    "    start_date_stock = end_date_stock + timedelta(-31)\n",
    "\n",
    "    # getting data from the API and adding to DataFrame\n",
    "    returns_df = get_historical_data(ticker, start_date_stock, end_date_stock,  \n",
    "                                     output_format='pandas')\n",
    "    returns_df.drop(columns=['open','high','low','volume'],inplace=True)\n",
    "    returns_df = returns_df.pct_change() * 100\n",
    "    returns_df.dropna(inplace=True)\n",
    "    returns_df.isnull().sum()\n",
    "    returns_df.rename(columns={'close':'return'}, inplace=True)\n",
    "    \n",
    "    combined_df = pd.concat([df, returns_df], axis=1)\n",
    "\n",
    "    # Function takes in an NLP/stock returns dataframe and calculates the average polarity\n",
    "    # scores over non-trading days and replaces scores of the first trading day after\n",
    "    # non-trading days with average score of previous days.\n",
    "    def cleaned_df(dataframe):\n",
    "        compound = []\n",
    "        positive = []\n",
    "        negative = []\n",
    "        neutral = []\n",
    "\n",
    "        for index, row in dataframe.iterrows():\n",
    "\n",
    "            # If daily return is null value for a given day, append polarity scores to their\n",
    "            # respective lists.\n",
    "            if pd.isnull(row['return']):\n",
    "                compound.append(row['compound'])\n",
    "                positive.append(row['positive'])\n",
    "                negative.append(row['negative'])\n",
    "                neutral.append(row['neutral'])\n",
    "                dataframe.drop(index=index, inplace=True)\n",
    "\n",
    "            elif pd.notnull(row['return']):\n",
    "                \n",
    "                # The list of compound polarity scores will be empty if the stock was traded\n",
    "                # on the previous day; therefore, move along.\n",
    "                if len(compound) == 0:\n",
    "                    pass\n",
    "\n",
    "                # If the list is not empty, then at least one day prior was a non-trading \n",
    "                # day. Append the current day's scores to the list and calculate the mean \n",
    "                # for each score. Then replace the current day's polarity scores with the \n",
    "                # average scores of today and previous non-trading days.\n",
    "                else:\n",
    "                    compound.append(row['compound'])\n",
    "                    compound_mean = np.mean(compound)\n",
    "                    compound = []\n",
    "\n",
    "                    positive.append(row['positive'])\n",
    "                    positive_mean = np.mean(positive)\n",
    "                    positive = []\n",
    "\n",
    "                    negative.append(row['negative'])\n",
    "                    negative_mean = np.mean(negative)\n",
    "                    negative = []\n",
    "\n",
    "                    neutral.append(row['neutral'])\n",
    "                    neutral_mean = np.mean(neutral)\n",
    "                    neutral = []\n",
    "\n",
    "                    dataframe.at[index, 'compound'] = compound_mean\n",
    "                    dataframe.at[index, 'positive'] = positive_mean\n",
    "                    dataframe.at[index, 'negative'] = negative_mean\n",
    "                    dataframe.at[index, 'neutral'] = neutral_mean\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return dataframe.sort_index(ascending=True)\n",
    "    \n",
    "    # Shift the return column up to adjust for a lag in stock reaction to sentiments.\n",
    "    final_df = cleaned_df(combined_df)\n",
    "    final_df['return'] = final_df['return'].shift(-lag)\n",
    "    final_df.dropna(inplace=True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn_df = create_df('AMZN', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>compound</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>markets smoother bufferfinancial market terrif...</td>\n",
       "      <td>0.897200</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.682000</td>\n",
       "      <td>2.783060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>whole foods market wednesday joined growing nu...</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>-1.852275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>reuters whole foods market employee working on...</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>3.073523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>washington reuters walmart inc said monday ha ...</td>\n",
       "      <td>0.912167</td>\n",
       "      <td>0.117667</td>\n",
       "      <td>0.100667</td>\n",
       "      <td>0.781667</td>\n",
       "      <td>1.958662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>wall street seeing green shoot tuesday market ...</td>\n",
       "      <td>0.984300</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>-2.796763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>photographer joshua lottbloombergattorneys gen...</td>\n",
       "      <td>-0.947700</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.117000</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>3.693314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>reuters ford motor co fn general motors co gmn...</td>\n",
       "      <td>0.986700</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>-2.832538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>reuters amazoncom inc amzno front line respond...</td>\n",
       "      <td>0.990600</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.765000</td>\n",
       "      <td>3.360349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>los angeles reuters kroger co walmart inc albe...</td>\n",
       "      <td>0.123467</td>\n",
       "      <td>0.093333</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>0.790667</td>\n",
       "      <td>-0.724560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>midmarch midnight shift one kristy granadoss c...</td>\n",
       "      <td>0.974700</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.851000</td>\n",
       "      <td>-2.155181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text  compound  \\\n",
       "date                                                                      \n",
       "2020-03-18  markets smoother bufferfinancial market terrif...  0.897200   \n",
       "2020-03-19  whole foods market wednesday joined growing nu...  0.996400   \n",
       "2020-03-20  reuters whole foods market employee working on...  0.995000   \n",
       "2020-03-23  washington reuters walmart inc said monday ha ...  0.912167   \n",
       "2020-03-24  wall street seeing green shoot tuesday market ...  0.984300   \n",
       "2020-03-25  photographer joshua lottbloombergattorneys gen... -0.947700   \n",
       "2020-03-26  reuters ford motor co fn general motors co gmn...  0.986700   \n",
       "2020-03-27  reuters amazoncom inc amzno front line respond...  0.990600   \n",
       "2020-03-30  los angeles reuters kroger co walmart inc albe...  0.123467   \n",
       "2020-03-31  midmarch midnight shift one kristy granadoss c...  0.974700   \n",
       "\n",
       "            positive  negative   neutral    return  \n",
       "date                                                \n",
       "2020-03-18  0.182000  0.137000  0.682000  2.783060  \n",
       "2020-03-19  0.184000  0.056000  0.759000 -1.852275  \n",
       "2020-03-20  0.147000  0.045000  0.808000  3.073523  \n",
       "2020-03-23  0.117667  0.100667  0.781667  1.958662  \n",
       "2020-03-24  0.125000  0.061000  0.815000 -2.796763  \n",
       "2020-03-25  0.078000  0.117000  0.805000  3.693314  \n",
       "2020-03-26  0.118000  0.052000  0.829000 -2.832538  \n",
       "2020-03-27  0.160000  0.076000  0.765000  3.360349  \n",
       "2020-03-30  0.093333  0.116000  0.790667 -0.724560  \n",
       "2020-03-31  0.099000  0.050000  0.851000 -2.155181  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amzn_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just lag function\n",
    "def df_lag(df, lag=0):\n",
    "    df['return'] = df['return'].shift(-lag)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
